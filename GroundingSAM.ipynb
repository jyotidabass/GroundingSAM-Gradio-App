{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMcvXrLGC4NxJ6sVRSlyMAF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyotidabass/GroundingSAM-Gradio-App/blob/main/GroundingSAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OLvh1Kc49Sfi",
        "outputId": "c57f3074-d8ac-4676-b029-e8f3d6938535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://ed4e9771b82e5de76d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ed4e9771b82e5de76d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-2-c60a3731388b>\", line 47, in query_image\n",
            "    box = [torch.round(pred[\"boxes\"][0], decimals=2), torch.round(pred[\"boxes\"][1], decimals=2),\n",
            "IndexError: index 0 is out of bounds for dimension 0 with size 0\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-2-c60a3731388b>\", line 47, in query_image\n",
            "    box = [torch.round(pred[\"boxes\"][0], decimals=2), torch.round(pred[\"boxes\"][1], decimals=2),\n",
            "IndexError: index 0 is out of bounds for dimension 0 with size 0\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-2-c60a3731388b>\", line 48, in query_image\n",
            "    torch.round(pred[\"boxes\"][2], decimals=2), torch.round(pred[\"boxes\"][3], decimals=2)]\n",
            "IndexError: index 3 is out of bounds for dimension 0 with size 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://ed4e9771b82e5de76d.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#!pip install spaces\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "import torch\n",
        "from transformers import SamModel, SamProcessor\n",
        "import spaces\n",
        "import numpy as np\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(\"cuda\")\n",
        "sam_processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
        "\n",
        "model_id = \"IDEA-Research/grounding-dino-base\"\n",
        "\n",
        "dino_processor = AutoProcessor.from_pretrained(model_id)\n",
        "dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
        "\n",
        "def infer_dino(img, text_queries, score_threshold):\n",
        "  queries=\"\"\n",
        "  for query in text_queries:\n",
        "    queries += f\"{query}. \"\n",
        "\n",
        "  width, height = img.shape[:2]\n",
        "\n",
        "  target_sizes=[(width, height)]\n",
        "  inputs = dino_processor(text=queries, images=img, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = dino_model(**inputs)\n",
        "    outputs.logits = outputs.logits.cpu()\n",
        "    outputs.pred_boxes = outputs.pred_boxes.cpu()\n",
        "    results = dino_processor.post_process_grounded_object_detection(outputs=outputs, input_ids=inputs.input_ids,\n",
        "                                                                  box_threshold=score_threshold,\n",
        "                                                                  target_sizes=target_sizes)\n",
        "  return results\n",
        "\n",
        "\n",
        "@spaces.GPU\n",
        "def query_image(img, text_queries, dino_threshold):\n",
        "  text_queries = text_queries\n",
        "  text_queries = text_queries.split(\",\")\n",
        "  dino_output = infer_dino(img, text_queries, dino_threshold)\n",
        "  result_labels=[]\n",
        "  for pred in dino_output:\n",
        "    boxes = pred[\"boxes\"].cpu()\n",
        "    scores = pred[\"scores\"].cpu()\n",
        "    labels = pred[\"labels\"]\n",
        "    box = [torch.round(pred[\"boxes\"][0], decimals=2), torch.round(pred[\"boxes\"][1], decimals=2),\n",
        "        torch.round(pred[\"boxes\"][2], decimals=2), torch.round(pred[\"boxes\"][3], decimals=2)]\n",
        "    for box, score, label in zip(boxes, scores, labels):\n",
        "      if label != \"\":\n",
        "        inputs = sam_processor(\n",
        "                img,\n",
        "                input_boxes=[[[box]]],\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = sam_model(**inputs)\n",
        "\n",
        "        mask = sam_processor.image_processor.post_process_masks(\n",
        "            outputs.pred_masks.cpu(),\n",
        "            inputs[\"original_sizes\"].cpu(),\n",
        "            inputs[\"reshaped_input_sizes\"].cpu()\n",
        "        )[0][0][0].numpy()\n",
        "        mask = mask[np.newaxis, ...]\n",
        "        result_labels.append((mask, label))\n",
        "  return img, result_labels\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "description = \"This Space combines [GroundingDINO](https://huggingface.co/IDEA-Research/grounding-dino-base), a bleeding-edge zero-shot object detection model with [SAM](https://huggingface.co/facebook/sam-vit-base), the state-of-the-art mask generation model. SAM normally doesn't accept text input. Combining SAM with OWLv2 makes SAM text promptable. Try the example or input an image and comma separated candidate labels to segment.\"\n",
        "demo = gr.Interface(\n",
        "    query_image,\n",
        "    inputs=[gr.Image(label=\"Image Input\"), gr.Textbox(label = \"Candidate Labels\"), gr.Slider(0, 1, value=0.05, label=\"Confidence Threshold for GroundingDINO\")],\n",
        "    outputs=\"annotatedimage\",\n",
        "    title=\"GroundingDINO 🤝 SAM for Zero-shot Segmentation\",\n",
        "    description=description,\n",
        "    examples=[\n",
        "        [\"./cats.png\", \"cat, fishnet\", 0.16],[\"./bee.jpg\", \"bee, flower\", 0.16]\n",
        "    ],\n",
        ")\n",
        "demo.launch(debug=True)"
      ]
    }
  ]
}